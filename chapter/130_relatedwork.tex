\chapter{Related Work}
\label{sec.relatedwork}

This chapter summarizes related work that investigates the inclusion of externally hosted JavaScript code, the security implications, and mitigation attempts. Furthermore, it presents security research related to JavaScript vulnerabilities. The former topics will be relevant for the threat model of this thesis.

\citeauthor{JSinclusions}~\cite{JSinclusions} conducted a large-scale evaluation of remote JavaScript inclusions in the most popular \num[round-precision=0]{10000} domains of the Alexa ranking over a period of ten years. As part of the evaluation, the researchers downloaded yearly snapshots of web pages hosted on these domains and processed more than three million pages in total. They analyzed the trust relationships between the sites and the providers of the remote scripts that they include and also assessed the maintenance quality of those providers based on metrics such as the security rating of the \acs{ssl}/\acs{tls} configuration and whether the server software was up-to-date.

The results of the evaluation of \citeauthor{JSinclusions} show that it is common practice for sites to include remote code, with \SI[round-precision=2]{88.45}{\percent} of the domains including at least one JavaScript library form a third party. Even though the majority of domains only include code from a few remote hosts, a small amount of popular sites include JavaScript libraries from up to \num[round-precision=0]{295} different remote hosts. It was also discovered that the amount of remote inclusions is steadily growing, reaching an average of \SI[round-precision=2]{2.10}{\percent} new unique domains per year in 2010. The study came to the conclusion that the inclusion of remote JavaScript libraries increases the attack surface and poses a threat to the security of web applications. When developers include remote JavaScript code in their web applications, they implicitly trust the providers of this code, since the latter have complete control over it and could execute malicious code at any time. This would give attackers the ability to steal user credentials or deface the site. The analysis regarding the quality of maintenace of the providers identified that a considerable amount of popular websites include code from third-parties that did not meet the best practices and security-standards imposed by the authors, which could lead them to be compromised by attackers. \cite{JSinclusions}

Another study \cite{ThirdPartyResources} analyzed the dependency chains created by third-party resources that include additional third-party resources themselves. Web applications that have these dependency chains place implicit trust on transitively included resources, which further increases the attack surface. Out of the \num[round-precision=0]{200000} most popular websites that were analyzed, \SI[round-precision=0]{91}{\percent} included JavaScript code from external hosts and around half of the websites implicitly include resources through a chain of dependencies. While \SI[round-precision=2]{84.91}{\percent} of the dependency chains do not exceed a length of three levels, the longest dependency chain reached a total length of \num[round-precision=0]{38} levels. The authors argue that complex chains of transitive inclusions make it difficult to reliably audit web applications, since is not possible to guarantee which resources will be included later on. The study also analyzed the third paries using VirusTotal and classified \SI[round-precision=2]{1.20}{\percent} of them as suspicious. The majority~(\SI[round-precision=0]{73}{\percent}) of websites that include resources from suspicious third-parties included them explicitly, which leads the authors to the conclusion that website operators are not thoroughly monitoring their external dependencies.

\citeauthor*{InBrowserDetection} \cite{InBrowserDetection} noted that, with increasing frequency, third-party content such as advertisements is also being injected by \acp{isp} and browser extensions for monetary benefits. Meanwhile, attackers use these advertisements to distribute malicious code. To defend against these types of injections, the authors developed an approach called “Excision”, which consists of modifications to the source code of the Chromium browser that analyzes the sequence of inclusions in order to identify and block malicious third-party content. Unlike traditional approaches, this method does not analyze the content of third-party resources, only the chain of inclusions that lead to the inclusion of the content in question. This method allows to combat common obfuscation techniques that attackers use to evade detection. To verify the effectiveness of their approach, an evaluation was performed, in which the modified browser analyzed the domains of the Alexa Top 200K ranking list over a period of eleven months. The results of the evaluation show that it has a high detection rate of \SI[round-precision=2]{93.39}{\percent} while maintaining a low false positive rate of \SI[round-precision=2]{0.59}{\percent}. An evaluation of the impact on performance showed that users did not notice a significant impact on their browsing experience.

To defend against the inclusion of malicious third-party content, \citeauthor*{InBrowserDetection} recommended the use of in-browser detection algorithms such as Excision in addition to techniques such as the \ac{csp}. They noted, however, that the \ac{csp} can be defeated by \acp{isp} and browser extensions since both of these parties are able to modify or remove the headers that are used to define this policy.

Additionally, there is research that focuses on JavaScript vulnerabilities. In \cite{Steffens_2021} Steffens discusses various client-side vulnerabilities, including Prototype Pollution. The thesis identified the presence of investigated vulnerabilities even among the most popular sites and concluded that there is a need for automated tools to assist web developers in detecting vulnerabilities. \cite{Steffens_2021}
